{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Preparation_Library.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6SQ/3EhQ0crSZuWEKeVif",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pawcio93/House-Prices-Kaggle-/blob/master/Data_Preparation_Library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtLoLPmfnEd7",
        "colab_type": "text"
      },
      "source": [
        "# EXTERNAL LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJBd7d-4Xj3u",
        "colab_type": "code",
        "outputId": "61a3daad-d8e7-4a0c-8842-059520ebb223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "### IMPORT LIBRARIES ###\n",
        "# MATH\n",
        "import math\n",
        "# NUMPY\n",
        "import numpy as np # linear algebra\n",
        "# PANDAS\n",
        "import pandas as pd # data processing\n",
        "pd.set_option('display.max_rows', 1000)\n",
        "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
        "# MATPLOTLIB\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pylab\n",
        "# SEABORN\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "sns.set_style('darkgrid')\n",
        "# SCIPY\n",
        "from scipy import stats\n",
        "from scipy.stats import norm, skew #for some statistics\n",
        "from scipy.special import boxcox1p\n",
        "# WARNING\n",
        "import warnings\n",
        "def ignore_warn(*args, **kwargs):\n",
        "    pass\n",
        "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
        "# SKLEARN\n",
        "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
        "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
        "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, make_scorer\n",
        "# XGBOOST\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import XGBRegressor\n",
        "# LIGTHGBM\n",
        "from lightgbm import LGBMRegressor\n",
        "# MODEL HELPERS\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn import feature_selection\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "# DISPLAY\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "from IPython.display import display_html\n",
        "# POWER PREDICTIVE SCORE\n",
        "!pip install ppscore\n",
        "import ppscore as pps\n",
        "# TABULETE\n",
        "!pip install tabulate\n",
        "from tabulate import tabulate\n",
        "# TENSORBOARD\n",
        "%load_ext tensorboard\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "# GRIDSEARCHCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# COLAB\n",
        "from google.colab import files\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ppscore in /usr/local/lib/python3.6/dist-packages (0.0.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (0.8.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0T1SPbOnJvJ",
        "colab_type": "text"
      },
      "source": [
        "# NOTEBOOK SETTINGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-pDw44gnWQ_",
        "colab_type": "text"
      },
      "source": [
        "# TEST DATASET\n",
        "### HOUSE PRICES DATASET (KAGGLE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbHza9CIlqRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### IMPORT AND PREPARE DATASETS ###\n",
        "dataset_train = pd.read_csv('train.csv')\n",
        "dataset_test = pd.read_csv('test.csv')\n",
        "data_description = open(\"data_description.txt\", \"r\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSA_OPbnlsYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a copy of train and test datasets\n",
        "X_train = dataset_train.copy()\n",
        "X_test = dataset_test.copy()\n",
        "X_set = [X_train, X_test]\n",
        "y_train = X_train['SalePrice']\n",
        "sub_id = X_test['Id']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAaR3gRmnjUV",
        "colab_type": "text"
      },
      "source": [
        "# CLASSES USED IN NOTEBOOK\n",
        "### - Processing class\n",
        "### - Data analysis class\n",
        "### - Transform class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEYcBQmhBliC",
        "colab_type": "text"
      },
      "source": [
        "## PROCESSING CLASS\n",
        "### - DataProcessing()\n",
        " - drop_vars_list () - Create a list of variables to drop, according to non values and overhelming frequency of one value in series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4zh7SqbiyQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### PROCESSING CLASS ####\n",
        "class DataProcessing(): \n",
        "  def __init__(self, X_train, X_test):\n",
        "    self.X_train = X_train # Train set\n",
        "    self.X_test = X_test # Test set\n",
        "    self.drop_list = [] # List of irrelevant columns\n",
        "    self.cleaning_list = [] # List of columns with nan values\n",
        "    self.datasets = [self.X_train, self.X_test] # list of datasets\n",
        "\n",
        "  def drop_vars_list(self, freq = 0.9, null_factor = 0.6):\n",
        "    # Checking if there is a value representing over (freq) of the column \n",
        "    # freq - frequency of value from 0 to 1, 0.5 means that half of samples have one value\n",
        "    # null_factor - parameter from 0 to 1, 0.5 means that half of samples have no value\n",
        "    rows = self.X_train.shape[0]\n",
        "    for column in self.X_train.columns:\n",
        "        x = self.X_train[column].value_counts()\n",
        "        frequence = x.iloc[0]/rows  \n",
        "        if frequence >= freq:\n",
        "            self.drop_list.append(column)\n",
        "    # Checking what percentage are nan value, if over (null_factor) - drop\n",
        "    for column in self.X_train.columns:\n",
        "        x = self.X_train[column].isnull().sum()\n",
        "        freq = x/rows\n",
        "        if freq >= null_factor:\n",
        "            self.drop_list.append(column)      \n",
        "    return print('drop_list CREATED: '), self.drop_list\n",
        "\n",
        "  def drop(self):\n",
        "    # Drop columns from drop_list\n",
        "    self.X_train.drop(self.drop_list, axis=1, inplace = True)\n",
        "    self.X_test.drop(self.drop_list, axis=1, inplace = True)\n",
        "    return print (\"Columns removed\")\n",
        "\n",
        "  def cleaning_vars_list(self):\n",
        "    # Create a list of columns which need to be cleaned (have nan values)\n",
        "    for dataset in self.datasets:\n",
        "        for column in dataset.columns:    \n",
        "            if dataset[column].isnull().any() == True:\n",
        "                if column not in self.cleaning_list:\n",
        "                    self.cleaning_list.append(column)\n",
        "    return print('cleaning_list CREATED: '), self.cleaning_list\n",
        "\n",
        "  def fill_with_estimate_value(self, cleaned_column, coeff_column):\n",
        "    # Clean column for datasets\n",
        "    print('Train before: ',  self.X_train[cleaned_column].isnull().sum())\n",
        "    print('Test before: ', self.X_test[cleaned_column].isnull().sum())\n",
        "    # Change nan values to calculated values \n",
        "    for dataset in self.datasets:\n",
        "        # Coefficient based on most correlated variable\n",
        "        # - find most correlated numerical variable\n",
        "        # - calculate correlation of vars means\n",
        "        # - use this coefficient to estimate nan values\n",
        "        coeff_1 = dataset[cleaned_column].mean()/dataset[coeff_column].mean()\n",
        "        for i in range(0, len(dataset)):\n",
        "            if np.isnan(dataset[cleaned_column][i]):\n",
        "                dataset[cleaned_column][i] = round(dataset[coeff_column][i]*coeff_1, 0)\n",
        "            else:\n",
        "                continue\n",
        "    print('Train after: ', self.X_train[cleaned_column].isnull().sum())\n",
        "    print('Test after: ', self.X_test[cleaned_column].isnull().sum())\n",
        "\n",
        "  def fill_with_most_common_value(self, column):  \n",
        "    # Checking number of nan values\n",
        "    print('Train before: ',  self.X_train[column].isnull().sum())\n",
        "    print('Test before: ', self.X_test[column].isnull().sum())\n",
        "    value = self.X_train[column].value_counts().idxmax()\n",
        "    # Filling missing values with most common value\n",
        "    for dataset in self.datasets:\n",
        "        dataset[column].fillna(value, inplace = True)     \n",
        "    print('Train after: ', self.X_train[column].isnull().sum())\n",
        "    print('Test after: ', self.X_test[column].isnull().sum())\n",
        "\n",
        "  def fill_with_median(self, column, dependent_column = 0, excluded_values = [], replace_excluded = 0):\n",
        "    # Checking number of nan values\n",
        "    # column - variable which we want to clean\n",
        "    # dependent_column - variable which we are based on while choosing string values\n",
        "    # excluded_values - list o values which should not be filled with median\n",
        "    # replace_excluded - value to replace excluded values\n",
        "    print('Train before: ',  self.X_train[column].isnull().sum())\n",
        "    print('Test before: ', self.X_test[column].isnull().sum())\n",
        "    # Filling missing values with median\n",
        "    for dataset in X_set:\n",
        "      if dependent_column == 0:\n",
        "        # Classic fill with median\n",
        "        dataset[column].fillna(dataset[column].median(),\n",
        "                                inplace = True)\n",
        "      else:\n",
        "        # Fill with median, but with exeptions\n",
        "        # example - column contain area of garage, in dependent column there is an\n",
        "        # information that there is no garage, hence area of garage is 0 not median\n",
        "        for i in dataset[dependent_column]:\n",
        "          if i in excluded_values:\n",
        "              dataset[column].fillna(replace_excluded, inplace = True)\n",
        "          else:\n",
        "              dataset[column].fillna(dataset[column].median(),\n",
        "                                      inplace = True)\n",
        "    print('Train after: ', self.X_train[column].isnull().sum())\n",
        "    print('Test after: ', self.X_test[column].isnull().sum())\n",
        "\n",
        "  def fill_with_mean(self, column, dependent_column = 0, excluded_values = [], replace_excluded = 0):\n",
        "    # Checking number of nan values\n",
        "    # column - variable which we want to clean\n",
        "    # dependent_column - variable which we are based on while choosing string values\n",
        "    # excluded_values - list o values which should not be filled with mean\n",
        "    # replace_excluded - value to replace excluded values  \n",
        "    print('Train before: ',  self.X_train[column].isnull().sum())\n",
        "    print('Test before: ', self.X_test[column].isnull().sum())\n",
        "    # Filling missing values with mean\n",
        "    for dataset in X_set:\n",
        "      if dependent_column == 0:\n",
        "        # Classic fill with mean\n",
        "        dataset[column].fillna(dataset[column].mean(),\n",
        "                                inplace = True)\n",
        "      else:\n",
        "        # Fill with mean, but with exeptions\n",
        "        # example - column contain area of garage, in dependent column there is an\n",
        "        # information that there is no garage, hence area of garage is 0 not mean\n",
        "        for i in dataset[dependent_column]:\n",
        "          if i in excluded_values:\n",
        "              dataset[column].fillna(0, inplace = True)\n",
        "          else:\n",
        "              dataset[column].fillna(dataset[column].mean(),\n",
        "                                      inplace = True)\n",
        "    print('Train after: ', self.X_train[column].isnull().sum())\n",
        "    print('Test after: ', self.X_test[column].isnull().sum())\n",
        "\n",
        "  def fill_with_strings(self, column, dependent_column, condition_list, value_list, last_value = 'NOT FILLED'):\n",
        "    # filling nan values with prepared string values, based on another column\n",
        "    # column - variable which we want to clean\n",
        "    # dependent_column - variable which we are based on while choosing string values\n",
        "    # condition_list - list of values from dependent_column, according to which we change nan values\n",
        "    # value_list - list of string used to fill nan\n",
        "    # last_value - if there is any nan not covered by list, fill it with this value\n",
        "    # Checking number of nan values         \n",
        "    print('Train before: ', self.X_train[column].isnull().sum())\n",
        "    print('Test before: ', self.X_test[column].isnull().sum())\n",
        "    # filling missing values based on chosen class\n",
        "    self.X_train[column].fillna(1, inplace = True)\n",
        "    self.X_test[column].fillna(1, inplace = True)\n",
        "    for dataset in self.datasets: # go into dataset\n",
        "      for value, condition in zip(value_list, condition_list):\n",
        "        for i in range(0, len(dataset)): \n",
        "            if dataset[dependent_column][i] == condition :\n",
        "                if dataset[column][i] == 1:\n",
        "                    dataset[column][i] = value\n",
        "            else:\n",
        "                continue\n",
        "    # check if there is any nan left, if yes then fill with last_value\n",
        "    for dataset in self.datasets: # go into dataset\n",
        "      for i in range(0, len(dataset)): \n",
        "          if dataset[column][i] == 1:\n",
        "              dataset[column][i] = last_value\n",
        "    print('Train after: ', self.X_train[column].isnull().sum())\n",
        "    print('Test after: ', self.X_test[column].isnull().sum())\n",
        "\n",
        "  def fill_with(self, column, value):\n",
        "    # simple fill with value function\n",
        "    # Checking number of nan values         \n",
        "    print('Train before: ', self.X_train[column].isnull().sum())\n",
        "    print('Test before: ', self.X_test[column].isnull().sum())\n",
        "    # filling missing values with most common one\n",
        "    self.X_train[column].fillna(value, inplace = True)\n",
        "    self.X_test[column].fillna(value, inplace = True)\n",
        "    print('Train after: ', self.X_train[column].isnull().sum())\n",
        "    print('Test after: ', self.X_test[column].isnull().sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZzdyY2PBttr",
        "colab_type": "text"
      },
      "source": [
        "## DATA ANALYSIS CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s19RiD7mvGyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataAnalysis():\n",
        "  def __init__(self, X_train, X_test):\n",
        "    self.X_train = X_train # Train set\n",
        "    self.X_test = X_test # Test set\n",
        "  \n",
        "  def multi_scatter(self,x_list, y):\n",
        "    ## Create scatter plots for multiple variables\n",
        "    ## 3 plots in a row\n",
        "    ## NOW FINISHED\n",
        "    sns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\n",
        "    sns.set_context(\"paper\", font_scale=2)  \n",
        "    for x in range(0, len(x_list)):\n",
        "      if x == 0 or x % 3:\n",
        "        chart = sns.pairplot(data=self.X_train,\n",
        "        y_vars=[y],\n",
        "        x_vars=[x_list[x], x_list[x+1], x_list[x+2]],\n",
        "        height = 10)\n",
        "        plt.xticks(rotation = 45)\n",
        "        plt.show()\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "  # def multi_scatter_2(self,x_list, y):\n",
        "  #   ## Create scatter plots for multiple variables\n",
        "  #   ## 3 plots in a row\n",
        "  #   ## NOT FINISHED\n",
        "  #   sns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\n",
        "  #   sns.set_context(\"paper\", font_scale=2)  \n",
        "  #   for x in range(0, len(x_list)):\n",
        "  #     if x == 0 or x % 3:\n",
        "  #       chart = sns.pairplot(data=self.X_train,\n",
        "  #       y_vars=[y],\n",
        "  #       x_vars=[x_list[x], x_list[x+1], x_list[x+2]],\n",
        "  #       height = 10)\n",
        "  #       for ax in chart.axes.flat:\n",
        "  #           labels = ax.get_xticklabels()\n",
        "  #           ax.set_xticklabels(labels=labels, rotation=45)\n",
        "  #       plt.show()\n",
        "  #     else:\n",
        "  #       continue\n",
        "\n",
        "  def var_overview(self, var, target):\n",
        "    # Checking var data type \n",
        "    numerical = ['int64', 'float64', 'int32', 'float32', 'int16', 'float16']\n",
        "    other = ['object', 'str']\n",
        "    if self.X_train[var].dtypes in numerical:\n",
        "      # DATA TABLE\n",
        "      (mu_v, sigma_v) = norm.fit(self.X_train[var])\n",
        "      (mu_t, sigma_t) = norm.fit(self.X_train[target])\n",
        "      headers = ['DATA', 'MEAN', 'STANDARD DEVIATION']\n",
        "      table_data = [('Variable', mu_v, sigma_v),\n",
        "                    ('Target', mu_t, sigma_t)]\n",
        "      print(tabulate(table_data, headers=headers, tablefmt='grid', numalign='center'))\n",
        "      # PLOTS\n",
        "      fig, axs = plt.subplots(2, figsize = (10,20))\n",
        "      # Scatter plot\n",
        "      axs[0].scatter(self.X_train[var], self.X_train[target])\n",
        "      axs[0].set_title('Scatter')\n",
        "      axs[0].set_ylabel(target)\n",
        "      axs[0].set_xlabel(var)\n",
        "      # Histgram\n",
        "      sns.distplot(self.X_train[var], ax=axs[1], fit=norm)\n",
        "      axs[1].set_title('Histogram')\n",
        "      axs[1].set_xlabel(var)\n",
        "      for axs in fig.axes:\n",
        "        plt.sca(axs)\n",
        "        plt.xticks(rotation=45)\n",
        "      # Probability plot\n",
        "      fig2 = plt.figure(figsize = (10.30,5))\n",
        "      res = stats.probplot(self.X_train[var], plot=plt)\n",
        "    elif self.X_train[var].dtypes in other:\n",
        "      # PLOTS\n",
        "      fig, axs = plt.subplots(2, figsize = (10,20))\n",
        "      # Scatter plot\n",
        "      axs[0].scatter(self.X_train[var], self.X_train[target])\n",
        "      axs[0].set_title('Scatter')\n",
        "      axs[0].set_ylabel(target)\n",
        "      axs[0].set_xlabel(var)\n",
        "      # Boxplot\n",
        "      sns.boxplot(x=var, y=target, data=self.X_train, ax=axs[1])\n",
        "      axs[1].set_title('Boxplot')\n",
        "      axs[1].set_ylabel(target)\n",
        "      axs[1].set_xlabel(var)\n",
        "      for axs in fig.axes:\n",
        "        plt.sca(axs)\n",
        "        plt.xticks(rotation=45)\n",
        "    else:\n",
        "      self.X_train[var].dtypes\n",
        "      self.X_train[target].dtypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j9pvPDYB1aO",
        "colab_type": "text"
      },
      "source": [
        "## TRANSFORM CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PANJ3LCHl4lh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformData():\n",
        "  def __init__(self, X_train, X_test):\n",
        "    self.X_train = X_train # Train set\n",
        "    self.X_test = X_test # Test set\n",
        "    self.datasets = [self.X_train, self.X_test] # list of datasets\n",
        "    self.skewed_vars = []\n",
        "    self.numerical_columns = []\n",
        "    self.best_results = []\n",
        "\n",
        "  def normality_test(self, target, skew_factor = 0.75):\n",
        "    # Checking var data type \n",
        "    numerical = ['int64', 'float64', 'int32', 'float32', 'int16', 'float16']\n",
        "    other = ['object', 'str']\n",
        "    # Creating list of columns\n",
        "    column_list = list(self.X_train.columns.values)\n",
        "    # delete dependent variable\n",
        "    column_list.remove(target)\n",
        "    # Create list of numerical vars\n",
        "    for ds in self.datasets:\n",
        "      for column in column_list:\n",
        "        if ds[column].dtypes in numerical:\n",
        "          if column not in self.numerical_columns:\n",
        "            self.numerical_columns.append(column)\n",
        "          else:\n",
        "            continue\n",
        "        else:\n",
        "          continue\n",
        "      # Check the skew of numerical variable\n",
        "      self.skewed_vars = ds[self.numerical_columns].apply(lambda x: skew(x))\n",
        "      # Create dataframe of variables with their skew values\n",
        "      self.skewness = pd.DataFrame({'Skew' :self.skewed_vars})\n",
        "      self.skewness = self.skewness[abs(self.skewness) > skew_factor].dropna()\n",
        "    print(\"Number of skewed variables {}\".format(self.skewness.shape[0]))\n",
        "    print(self.numerical_columns)\n",
        "\n",
        "  def skew_fix(self, lmbda = 0.5):\n",
        "    skews = self.skewness.index\n",
        "    for ds in self.datasets:\n",
        "      for skew in skews:\n",
        "          # Apply transformation methods\n",
        "          ds_boxcox = boxcox1p(ds[skew], lmbda)\n",
        "          ds_log = np.log(ds[skew])\n",
        "          ds_square = np.sqrt(ds[skew])\n",
        "          # Check which method have the best result\n",
        "          box_cox_skew = pd.Series(ds_boxcox).skew()\n",
        "          log_skew = pd.Series(ds_log).skew()\n",
        "          square_skew = pd.Series(ds_square).skew()\n",
        "          results = [box_cox_skew,\n",
        "                     log_skew,\n",
        "                     square_skew]\n",
        "          self.best_results.append(results.index(min(results)))\n",
        "    for ds in self.datasets:          \n",
        "      for best, skew in zip(self.best_results, skews):\n",
        "          if best == 0:\n",
        "            ds[skew] = boxcox1p(ds[skew], lmbda)\n",
        "          elif best == 1:\n",
        "            ds[skew] = np.log(ds[skew])\n",
        "          else:\n",
        "            ds[skew] = np.sqrt(ds[skew])\n",
        "\n",
        "  def data_skalling(self):\n",
        "    scaler = RobustScaler() \n",
        "    for ds in self.datasets:\n",
        "      #  ds[self.numerical_columns] = scaler.fit_transform(ds[self.numerical_columns]) \n",
        "       ds[self.numerical_columns] = scaler.fit_transform(ds[self.numerical_columns]) \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}